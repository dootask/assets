#!/usr/bin/env python3
"""
AIèŠå¤©æ¥å£ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨åŸºäºLangChainçš„AIèŠå¤©æ¥å£ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€èŠå¤©
2. å¤šè½®å¯¹è¯
3. çŸ¥è¯†åº“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰
4. MCPå·¥å…·è°ƒç”¨
5. æµå¼å“åº”

è¿è¡Œå‰è¯·ç¡®ä¿ï¼š
1. å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install -r requirements.txt
2. é…ç½®äº†ç›¸åº”çš„APIå¯†é’¥
3. å¯åŠ¨äº†AIæœåŠ¡ï¼špython -m app.main
"""

import asyncio
import json
import os
from typing import Any, Dict

import httpx

# æœåŠ¡åŸºç¡€URL
BASE_URL = "http://localhost:8001"  # æ ¹æ®å®é™…ç«¯å£è°ƒæ•´

# api_key
API_KEY = os.getenv("OPENAI_API_KEY", "")


class ChatClient:
    """èŠå¤©å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = BASE_URL):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=30.0)
    
    async def chat(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‘é€èŠå¤©è¯·æ±‚"""
        response = await self.client.post(
            f"{self.base_url}/chat",
            json=request_data
        )
        response.raise_for_status()
        return response.json()
    
    async def stream_chat(self, request_data: Dict[str, Any]):
        """å‘é€æµå¼èŠå¤©è¯·æ±‚"""
        async with self.client.stream(
            "POST",
            f"{self.base_url}/chat/stream",
            json=request_data
        ) as response:
            response.raise_for_status()
            
            event_type = None
            
            async for line in response.aiter_lines():
                line = line.strip()
                
                if line.startswith("event: "):
                    event_type = line[7:]  # ç§»é™¤ "event: " å‰ç¼€
                elif line.startswith("data: "):
                    data = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                    if data.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºè¡Œ
                        try:
                            parsed_data = json.loads(data)
                            # å°†eventç±»å‹æ·»åŠ åˆ°æ•°æ®ä¸­ä»¥ä¿æŒå…¼å®¹æ€§
                            if event_type:
                                parsed_data['type'] = event_type
                            yield parsed_data
                            event_type = None  # é‡ç½®äº‹ä»¶ç±»å‹
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸  JSONè§£æé”™è¯¯: {line} -> {data} - {e}")
                            continue
                elif line == "":
                    # ç©ºè¡Œè¡¨ç¤ºäº‹ä»¶ç»“æŸï¼Œé‡ç½®çŠ¶æ€
                    event_type = None
    
    async def get_models(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        response = await self.client.get(f"{self.base_url}/chat/models")
        response.raise_for_status()
        return response.json()
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


async def example_basic_chat():
    """ç¤ºä¾‹1: åŸºç¡€èŠå¤©"""
    print("=== ç¤ºä¾‹1: åŸºç¡€èŠå¤© ===")
    
    client = ChatClient()
    
    # æ„å»ºè¯·æ±‚
    request = {
        "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "generation_config": {
            "max_tokens": 1000,
            "temperature": 0.7
        },
        "system_message": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
    }
    
    try:
        response = await client.chat(request)
        print(f"AIå›å¤: {response['message']}")
        print(f"ä½¿ç”¨æ¨¡å‹: {response['provider']}/{response['model']}")
        print(f"å“åº”æ—¶é—´: {response['timestamp']}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_multi_turn_conversation():
    """ç¤ºä¾‹2: å¤šè½®å¯¹è¯"""
    print("\n\n=== ç¤ºä¾‹2: å¤šè½®å¯¹è¯ ===")
    
    client = ChatClient()
    
    # å¯¹è¯å†å²
    conversation_history = [
        {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹"},
        {"role": "assistant", "content": "å¾ˆå¥½ï¼Pythonæ˜¯ä¸€é—¨éå¸¸é€‚åˆåˆå­¦è€…çš„ç¼–ç¨‹è¯­è¨€ã€‚ä½ æƒ³ä»å“ªä¸ªæ–¹é¢å¼€å§‹å­¦ä¹ å‘¢ï¼Ÿ"},
        {"role": "user", "content": "æˆ‘æƒ³å…ˆå­¦ä¹ åŸºç¡€è¯­æ³•"}
    ]
    
    request = {
        "messages": conversation_history,
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "conversation_id": "conv_001"
    }
    
    try:
        response = await client.chat(request)
        print(f"AIå›å¤: {response['message']}")
        print(f"å¯¹è¯ID: {response['conversation_id']}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_rag_chat():
    """ç¤ºä¾‹3: çŸ¥è¯†åº“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰"""
    print("\n\n=== ç¤ºä¾‹3: çŸ¥è¯†åº“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "DooTaské¡¹ç›®çš„ä¸»è¦åŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "retrieval_config": {
            "enabled": True,
            "knowledge_base_ids": ["kb_dootask_docs", "kb_project_info"],
            "top_k": 3,
            "score_threshold": 0.7,
            "rerank": True
        }
    }
    
    try:
        response = await client.chat(request)
        print(f"AIå›å¤: {response['message']}")
        
        if response.get('retrieval_docs'):
            print("\næ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(response['retrieval_docs'], 1):
                print(f"{i}. æ¥æº: {doc['source']}")
                print(f"   å†…å®¹: {doc['content'][:100]}...")
                print(f"   ç›¸ä¼¼åº¦: {doc['score']}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_mcp_tools():
    """ç¤ºä¾‹4: MCPå·¥å…·è°ƒç”¨"""
    print("\n\n=== ç¤ºä¾‹4: MCPå·¥å…·è°ƒç”¨ ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "è¯·å¸®æˆ‘æŸ¥è¯¢ä»Šå¤©çš„å¤©æ°”ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæé†’ä»»åŠ¡",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "mcp_config": {
            "enabled": True,
            "tools": [
                {
                    "name": "weather_tool",
                    "enabled": True,
                    "config": {"api_key": "weather-api-key"}
                },
                {
                    "name": "task_manager",
                    "enabled": True,
                    "config": {"workspace_id": "ws_123"}
                }
            ],
            "tool_choice": "auto",
            "max_tool_calls": 3
        }
    }
    
    try:
        response = await client.chat(request)
        print(f"AIå›å¤: {response['message']}")
        
        if response.get('tool_calls'):
            print("\nå·¥å…·è°ƒç”¨è®°å½•:")
            for tool_call in response['tool_calls']:
                print(f"- å·¥å…·: {tool_call['tool_name']}")
                print(f"  ç»“æœ: {tool_call['result']}")
                print(f"  çŠ¶æ€: {'æˆåŠŸ' if tool_call['success'] else 'å¤±è´¥'}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_streaming_chat():
    """ç¤ºä¾‹5: æµå¼å“åº”"""
    print("\n\n=== ç¤ºä¾‹5: æµå¼å“åº” ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„å‘å±•å†ç¨‹",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "stream": True,
        "generation_config": {
            "max_tokens": 2000,
            "temperature": 0.8
        }
    }
    
    try:
        print("AIæ­£åœ¨æ€è€ƒå¹¶å›å¤...")
        response_content = ""
        
        async for chunk in client.stream_chat(request):
            chunk_type = chunk.get('type')
            
            if chunk_type == 'start':
                print("å¼€å§‹ç”Ÿæˆå›å¤...")
                print(f"æ¶ˆæ¯: {chunk.get('message', '')}")
            elif chunk_type == 'token':
                content = chunk.get('content', '')
                response_content += content
                print(content, end='', flush=True)
            elif chunk_type == 'retrieval':
                docs = chunk.get('docs', [])
                print(f"\n[æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£]")
            elif chunk_type == 'tools':
                tool_calls = chunk.get('tool_calls', [])
                print(f"\n[è°ƒç”¨äº† {len(tool_calls)} ä¸ªå·¥å…·]")
            elif chunk_type == 'end':
                print(f"\n\nå›å¤å®Œæˆ! {chunk.get('message', '')}")
            elif chunk_type == 'error':
                print(f"\né”™è¯¯: {chunk.get('error')}")
                break
        
        print(f"\nå®Œæ•´å›å¤é•¿åº¦: {len(response_content)} å­—ç¬¦")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_streaming_with_rag_and_tools():
    """ç¤ºä¾‹6: æµå¼å“åº” + çŸ¥è¯†åº“æ£€ç´¢ + å·¥å…·è°ƒç”¨"""
    print("\n\n=== ç¤ºä¾‹6: æµå¼å“åº” + çŸ¥è¯†åº“æ£€ç´¢ + å·¥å…·è°ƒç”¨ ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "è¯·æ ¹æ®DooTaské¡¹ç›®æ–‡æ¡£ï¼Œå¸®æˆ‘æŸ¥è¯¢å½“å‰å¤©æ°”ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå…³äºé¡¹ç›®éƒ¨ç½²çš„ä»»åŠ¡æé†’",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "stream": True,
        "generation_config": {
            "max_tokens": 2000,
            "temperature": 0.7
        },
        "system_message": "ä½ æ˜¯DooTaské¡¹ç›®çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®¿é—®é¡¹ç›®æ–‡æ¡£ã€æŸ¥è¯¢å¤©æ°”å’Œç®¡ç†ä»»åŠ¡ã€‚",
        "retrieval_config": {
            "enabled": True,
            "knowledge_base_ids": ["kb_dootask_docs", "kb_deployment_guide"],
            "top_k": 3,
            "score_threshold": 0.7,
            "rerank": True
        },
        "mcp_config": {
            "enabled": True,
            "tools": [
                {
                    "name": "weather_tool",
                    "enabled": True,
                    "config": {
                        "api_key": "weather-api-key",
                        "default_location": "Beijing"
                    }
                },
                {
                    "name": "task_manager",
                    "enabled": True,
                    "config": {
                        "workspace_id": "ws_dootask",
                        "default_assignee": "admin"
                    }
                }
            ],
            "tool_choice": "auto",
            "max_tool_calls": 5
        }
    }
    
    try:
        print("ğŸš€ å¼€å§‹å¤„ç†å¤åˆè¯·æ±‚ï¼ˆæ£€ç´¢+å·¥å…·+æµå¼ç”Ÿæˆï¼‰...")
        response_content = ""
        retrieved_docs = []
        tool_results = []
        
        async for chunk in client.stream_chat(request):
            chunk_type = chunk.get('type')
            
            if chunk_type == 'start':
                print("âœ… è¿æ¥å»ºç«‹ï¼Œå¼€å§‹å¤„ç†...")
                
            elif chunk_type == 'retrieval':
                docs = chunk.get('docs', [])
                retrieved_docs.extend(docs)
                print(f"\nğŸ“š çŸ¥è¯†åº“æ£€ç´¢å®Œæˆ:")
                print(f"   - æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                for i, doc in enumerate(docs, 1):
                    print(f"   {i}. æ¥æº: {doc.get('source', 'unknown')}")
                    print(f"      å†…å®¹é¢„è§ˆ: {doc.get('content', '')[:80]}...")
                    print(f"      ç›¸ä¼¼åº¦: {doc.get('score', 0):.3f}")
                print("")
                
            elif chunk_type == 'token':
                content = chunk.get('content', '')
                response_content += content
                print(content, end='', flush=True)
                
            elif chunk_type == 'tools':
                tool_calls = chunk.get('tool_calls', [])
                tool_results.extend(tool_calls)
                print(f"\n\nğŸ”§ å·¥å…·è°ƒç”¨å®Œæˆ:")
                for tool_call in tool_calls:
                    tool_name = tool_call.get('tool_name', 'unknown')
                    success = tool_call.get('success', False)
                    result = tool_call.get('result', '')
                    
                    status_icon = "âœ…" if success else "âŒ"
                    print(f"   {status_icon} {tool_name}:")
                    print(f"      ç»“æœ: {result}")
                print("")
                
            elif chunk_type == 'end':
                print(f"\n\nğŸ‰ å¤„ç†å®Œæˆ! {chunk.get('message', '')}")
                
            elif chunk_type == 'error':
                print(f"\nâŒ é”™è¯¯: {chunk.get('error')}")
                break
        
        # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ“Š å¤„ç†æ€»ç»“:")
        print(f"   ğŸ’¬ ç”Ÿæˆå†…å®¹é•¿åº¦: {len(response_content)} å­—ç¬¦")
        print(f"   ğŸ“š æ£€ç´¢æ–‡æ¡£æ•°é‡: {len(retrieved_docs)} ä¸ª")
        print(f"   ğŸ”§ å·¥å…·è°ƒç”¨æ¬¡æ•°: {len(tool_results)} æ¬¡")
        
        if retrieved_docs:
            print(f"\nğŸ“‹ æ£€ç´¢æ–‡æ¡£è¯¦æƒ…:")
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"   {i}. {doc.get('source', 'unknown')} (ç›¸ä¼¼åº¦: {doc.get('score', 0):.3f})")
        
        if tool_results:
            print(f"\nğŸ› ï¸  å·¥å…·è°ƒç”¨è¯¦æƒ…:")
            for i, tool in enumerate(tool_results, 1):
                status = "æˆåŠŸ" if tool.get('success', False) else "å¤±è´¥"
                print(f"   {i}. {tool.get('tool_name', 'unknown')} - {status}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_streaming_knowledge_only():
    """ç¤ºä¾‹7: çº¯çŸ¥è¯†åº“æ£€ç´¢æµå¼å“åº”"""
    print("\n\n=== ç¤ºä¾‹7: çº¯çŸ¥è¯†åº“æ£€ç´¢æµå¼å“åº” ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "DooTaské¡¹ç›®çš„æ¶æ„è®¾è®¡æ˜¯æ€æ ·çš„ï¼Ÿè¯·è¯¦ç»†è¯´æ˜å„ä¸ªç»„ä»¶çš„ä½œç”¨ã€‚",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "api_key": API_KEY
        },
        "stream": True,
        "generation_config": {
            "max_tokens": 1500,
            "temperature": 0.5
        },
        "system_message": "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æã€‚",
        "retrieval_config": {
            "enabled": True,
            "knowledge_base_ids": ["kb_dootask_architecture", "kb_component_docs", "kb_api_docs"],
            "top_k": 5,
            "score_threshold": 0.6,
            "rerank": True
        }
    }
    
    try:
        print("ğŸ“– å¼€å§‹åŸºäºçŸ¥è¯†åº“çš„æµå¼é—®ç­”...")
        response_content = ""
        
        async for chunk in client.stream_chat(request):
            chunk_type = chunk.get('type')
            
            if chunk_type == 'start':
                print("ğŸ” å¼€å§‹æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
                
            elif chunk_type == 'retrieval':
                docs = chunk.get('docs', [])
                print(f"ğŸ“š æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œæ­£åœ¨åˆ†æ...")
                for doc in docs:
                    source = doc.get('source', 'unknown')
                    score = doc.get('score', 0)
                    print(f"   - {source} (åŒ¹é…åº¦: {score:.1%})")
                print("ğŸ¤– AIæ­£åœ¨åŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”...\n")
                
            elif chunk_type == 'token':
                content = chunk.get('content', '')
                response_content += content
                print(content, end='', flush=True)
                
            elif chunk_type == 'end':
                print(f"\n\nâœ… åŸºäºçŸ¥è¯†åº“çš„å›ç­”å®Œæˆ!")
                
            elif chunk_type == 'error':
                print(f"\nâŒ é”™è¯¯: {chunk.get('error')}")
                break
        
        print(f"\nğŸ“ˆ ç”Ÿæˆäº† {len(response_content)} å­—ç¬¦çš„è¯¦ç»†å›ç­”")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_streaming_tools_only():
    """ç¤ºä¾‹8: çº¯å·¥å…·è°ƒç”¨æµå¼å“åº”"""
    print("\n\n=== ç¤ºä¾‹8: çº¯å·¥å…·è°ƒç”¨æµå¼å“åº” ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "è¯·å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”ï¼Œç„¶ååˆ›å»º3ä¸ªä¸åŒä¼˜å…ˆçº§çš„ä»»åŠ¡æé†’",
        "model": {
            "provider": "openai",
            "model": "gpt-3.5-turbo", 
            "api_key": API_KEY
        },
        "stream": True,
        "generation_config": {
            "max_tokens": 1000,
            "temperature": 0.3
        },
        "system_message": "ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ä»»åŠ¡åŠ©æ‰‹ï¼Œèƒ½å¤ŸæŸ¥è¯¢å¤©æ°”å’Œç®¡ç†ä»»åŠ¡ã€‚",
        "mcp_config": {
            "enabled": True,
            "tools": [
                {
                    "name": "weather_tool",
                    "enabled": True,
                    "config": {
                        "api_key": "weather-api-key",
                        "units": "metric"
                    }
                },
                {
                    "name": "task_manager",
                    "enabled": True,
                    "config": {
                        "workspace_id": "ws_personal",
                        "default_due_hours": 24
                    }
                },
                {
                    "name": "calendar_tool", 
                    "enabled": True,
                    "config": {
                        "calendar_id": "primary"
                    }
                }
            ],
            "tool_choice": "required",  # å¼ºåˆ¶ä½¿ç”¨å·¥å…·
            "max_tool_calls": 8
        }
    }
    
    try:
        print("ğŸ”§ å¼€å§‹å¤šå·¥å…·åä½œæµå¼å¤„ç†...")
        response_content = ""
        tool_call_count = 0
        
        async for chunk in client.stream_chat(request):
            chunk_type = chunk.get('type')
            
            if chunk_type == 'start':
                print("ğŸš€ å¼€å§‹å·¥å…·è°ƒç”¨åºåˆ—...")
                
            elif chunk_type == 'tools':
                tool_calls = chunk.get('tool_calls', [])
                tool_call_count += len(tool_calls)
                print(f"\nğŸ”§ æ‰§è¡Œäº† {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨:")
                
                for i, tool_call in enumerate(tool_calls, 1):
                    tool_name = tool_call.get('tool_name', 'unknown')
                    success = tool_call.get('success', False)
                    result = tool_call.get('result', '')
                    
                    status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
                    print(f"   {i}. [{tool_name}] {status}")
                    print(f"      ğŸ“„ ç»“æœ: {result}")
                
                print("\nğŸ¤– AIæ­£åœ¨æ•´åˆå·¥å…·ç»“æœ...")
                
            elif chunk_type == 'token':
                content = chunk.get('content', '')
                response_content += content
                print(content, end='', flush=True)
                
            elif chunk_type == 'end':
                print(f"\n\nğŸ¯ å·¥å…·è°ƒç”¨æµç¨‹å®Œæˆ!")
                
            elif chunk_type == 'error':
                print(f"\nâŒ é”™è¯¯: {chunk.get('error')}")
                break
        
        print(f"\nğŸ“Š æ€»è®¡è°ƒç”¨äº† {tool_call_count} æ¬¡å·¥å…·ï¼Œç”Ÿæˆ {len(response_content)} å­—ç¬¦å›ç­”")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    
    await client.close()


async def example_ollama_local_model():
    """ç¤ºä¾‹9: ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹"""
    print("\n\n=== ç¤ºä¾‹9: ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ ===")
    
    client = ChatClient()
    
    request = {
        "prompt": "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
        "model": {
            "provider": "ollama",
            "model": "llama3",
            "base_url": "http://localhost:11434"  # Ollamaé»˜è®¤åœ°å€
        },
        "generation_config": {
            "temperature": 0.3
        },
        "system_message": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·æä¾›æ¸…æ™°çš„ä»£ç ç¤ºä¾‹ã€‚"
    }
    
    try:
        response = await client.chat(request)
        print(f"AIå›å¤: {response['message']}")
        print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {response['model']}")
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”å·²ä¸‹è½½äº†ç›¸åº”çš„æ¨¡å‹")
    
    await client.close()

async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¤– DooTask AIèŠå¤©æ¥å£ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åŸºç¡€åŠŸèƒ½ç¤ºä¾‹
    await example_basic_chat()
    await example_multi_turn_conversation()
    
    # é«˜çº§åŠŸèƒ½ç¤ºä¾‹  
    await example_rag_chat()
    await example_mcp_tools()
    
    # æµå¼å“åº”ç¤ºä¾‹
    await example_streaming_chat()
    await example_streaming_with_rag_and_tools()
    await example_streaming_knowledge_only()
    await example_streaming_tools_only()
    
    # æœ¬åœ°æ¨¡å‹ç¤ºä¾‹
    await example_ollama_local_model()
    
    print("\nâœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")


async def run_specific_examples():
    """è¿è¡Œç‰¹å®šç¤ºä¾‹"""
    examples = {
        "1": ("åŸºç¡€èŠå¤©", example_basic_chat),
        "2": ("å¤šè½®å¯¹è¯", example_multi_turn_conversation),
        "3": ("çŸ¥è¯†åº“æ£€ç´¢(RAG)", example_rag_chat),
        "4": ("MCPå·¥å…·è°ƒç”¨", example_mcp_tools),
        "5": ("åŸºç¡€æµå¼å“åº”", example_streaming_chat),
        "6": ("æµå¼å“åº”+RAG+å·¥å…·", example_streaming_with_rag_and_tools),
        "7": ("çº¯çŸ¥è¯†åº“æµå¼å“åº”", example_streaming_knowledge_only),
        "8": ("çº¯å·¥å…·æµå¼å“åº”", example_streaming_tools_only),
        "9": ("æœ¬åœ°Ollamaæ¨¡å‹", example_ollama_local_model),
    }
    
    print("ğŸ¤– DooTask AIèŠå¤©æ¥å£ä½¿ç”¨ç¤ºä¾‹")
    
    while True:
        print("\n" + "=" * 50)
        print("è¯·é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹:")

        for key, (name, _) in examples.items():
            print(f"  {key}. {name}")
    
        print("  0. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹")
        print("  q. é€€å‡º")
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-9 æˆ– q): ").strip()
        
        if choice.lower() == 'q':
            print("ğŸ‘‹ å†è§!")
            break
        elif choice == '0':
            await main()
            break
        elif choice in examples:
            name, func = examples[choice]
            print(f"\nğŸš€ è¿è¡Œç¤ºä¾‹: {name}")
            print("-" * 40)
            try:
                await func()
                print(f"\nâœ… ç¤ºä¾‹ '{name}' è¿è¡Œå®Œæˆ!")
            except Exception as e:
                print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå‡ºé”™: {e}")
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")


if __name__ == "__main__":
    import sys

    if not API_KEY:
        print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)

    # å¦‚æœæœ‰å‘½ä»¤è¡Œå‚æ•° --allï¼Œè¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    if len(sys.argv) > 1 and sys.argv[1] == "--all":
        asyncio.run(main())
    else:
        # å¦åˆ™è¿è¡Œäº¤äº’å¼é€‰æ‹©
        asyncio.run(run_specific_examples()) 